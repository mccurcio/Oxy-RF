---
title: "Random Forest for Mono-Percent AA Composition"
author: "Matthew Curcio"
date: "October 5, 2017"
output: html_document
---

```{r libraries}
library(randomForest)
library(readr)
library(foreach)
```

```{r import_dataset}
mono_aa_set <- read_csv("~/Dropbox/git_projects/random_forest/2_single_aa/total_mono_aa_heme_notheme.csv")

mono_aa_set$Class <- as.character(mono_aa_set$Class)
mono_aa_set$Class <- as.factor(mono_aa_set$Class)

head(mono_aa_set)
```

## Produce Training Data Set
* training Sample with 70% dataset observations
* 16954 * 0.7 = 11868

```{r}
set.seed(100)
train = sample(1:nrow(mono_aa_set), 11868)
head(train)
```

```{r}
# Original Data
table(data$Edible)/nrow(data)  
##    Edible Poisonous 
## 0.5179714 0.4820286
# Training Data
table(data.dev$Edible)/nrow(data.dev)  
##    Edible Poisonous 
## 0.4962779 0.5037221
# Testing Data
table(data.val$Edible)/nrow(data.val)  
##    Edible Poisonous 
## 0.5191037 0.4808963
```



We will use all the Predictors in the dataset.
## Fitting the Random Forest
```{r train_rf}
# Start the clock!
ptm <- proc.time()

# mono_aa_set$Class =  as.factor(mono_aa_set$Class)
mono_aa_set.rf = randomForest(mono_aa_set$Class ~ ., 
                              data = mono_aa_set, 
                              subset = train)
# Stop the clock
proc.time() - ptm
```

## Show Results from Training Run
```{r results_1}
mono_aa_set.rf
```

```{r}
mono_aa_set.rf$importance
```


```{r plot_1}
plot(mono_aa_set.rf)
```

* Now we can compare the *Out of Bag Sample Errors* and *Error on Test set*

The above Random Forest model randomly chose 4 variables to be considered at each split. We could now try all possible predictors (1 to 21) which can be found at each split.
```{r}
oob.err = double(21)
test.err = double(21)
```

### mtry is number of Variables randomly chosen at each split

```{r mtry_all}
# Start the clock!
ptm <- proc.time()

mono_aa_set$Class <- as.character(mono_aa_set$Class)
class_response <- as.factor(mono_aa_set$Class)

for(mtry in 1:3) {
    rf = randomForest(Class ~ ., data = mono_aa_set , subset = train,
                      mtry = mtry, ntree = 300)

# for(mtry in 1:3) {
#     rf = randomForest(mono_aa_set$Class ~ ., 
#                       y = class_response,
#                       data = mono_aa_set , 
#                       importance = TRUE,
#                       proximity = TRUE,
#                       subset = train,
#                       nodesize = if (!is.null(y) && !is.factor(y)) 5 else 1,
#                       mtry = mtry,
#                       ntree = 300) 
    
    oob.err[mtry] = rf$mse[300] 
    #Error of all Trees fitted
    
    pred = predict(rf, mono_aa_set[-train,]) 
    #Predictions on Test Set foClassr each Tree
    
    test.err[mtry] = with(mono_aa_set[-train,], 
                          mean((mono_aa_set$Class - pred)^2))     
    #Mean Squared Test Error
    
    cat(mtry," ") #printing the output to the console
    
}

# Stop the clock
(proc.time() - ptm)/60
```


## Test Error
```{r}
test.err
```


## Out of Bag Error Estimation
```{r}
oob.err
```

## Plotting Both Test Error and Out of Bag Error
```{r plot_2}
matplot(1:mtry, 
        cbind(oob.err, test.err), 
        pch = 19, 
        col = c("red", "blue"),
        type = "b",
        ylab = "Mean Squared Error",
        xlab = "Number of Predictors Considered at each Split")
legend("topright",
       legend  =c("Out of Bag Error", "Test Error"),
       pch = 19, 
       col  =c("red", "blue"))
```

